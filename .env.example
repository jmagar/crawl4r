# RAG Ingestion Pipeline Configuration Template
# Copy this file to .env and configure for your environment

# REQUIRED: Directory to watch for markdown files
WATCH_FOLDER=

# TEI Embeddings Service Endpoint
# Default: http://crawl4r-embeddings:80
TEI_ENDPOINT=http://crawl4r-embeddings:80

# Qdrant Vector Database URL
# Default: http://crawl4r-vectors:6333
QDRANT_URL=http://crawl4r-vectors:6333

# Qdrant Collection Name
# Default: crawl4r
COLLECTION_NAME=crawl4r

# Chunking Configuration
# Default: 512 tokens per chunk
CHUNK_SIZE_TOKENS=512

# Chunk overlap percentage (0-50)
# Default: 15 (77 tokens for 512-token chunks)
CHUNK_OVERLAP_PERCENT=15

# Performance Settings
# Maximum concurrent documents to process
# Default: 10
MAX_CONCURRENT_DOCS=10

# Maximum queue size before backpressure
# Default: 1000
QUEUE_MAX_SIZE=1000

# Batch size for embedding requests
# Default: 32
BATCH_SIZE=32

# Logging Configuration
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO
LOG_LEVEL=INFO

# Failed documents log file path
# Default: ./failed_documents.jsonl
FAILED_DOCS_LOG=./failed_documents.jsonl

# Crawl4AI LLM provider configuration
# Set only the provider(s) you plan to use.
OPENAI_API_KEY=
DEEPSEEK_API_KEY=
ANTHROPIC_API_KEY=
GROQ_API_KEY=
TOGETHER_API_KEY=
MISTRAL_API_KEY=
GEMINI_API_KEY=

# Optional: override default provider, e.g. "anthropic/claude-3-opus"
LLM_PROVIDER=
