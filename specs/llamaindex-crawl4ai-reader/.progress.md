---
spec: llamaindex-crawl4ai-reader
phase: research
task: 0/0
updated: 2026-01-15T00:00:00Z
---

# Progress: llamaindex-crawl4ai-reader

## Original Goal

Create a LlamaIndex reader for Crawl4AI so we can wire it up to the rest of our pipeline we just finished implementing. We currently have Crawl4AI deployed via Docker.

## Completed Tasks

- [x] 1.1.1 Add httpx dependency - 01552f4
- [x] 1.1.2 Verify respx test dependency - verified (no commit needed)
- [x] 1.2.1 Add source_url to PAYLOAD_INDEXES - a2ace3c
- [x] 1.2.2 Implement delete_by_url method - 76febcf
- [x] V1 Quality checkpoint: vector_store changes - 3e0faa5
- [x] 1.3.1 Create crawl4ai_reader module - 101d19f
- [x] 1.3.2 Create unit test file - 4786bc5
- [x] 1.3.3 Create integration test file - 6f1014a
- [x] 1.3.4 Create test fixtures file - bcdcea6
- [x] 1.4.1 RED: Test Settings integration - 9ba39b8
- [x] 1.4.2 GREEN: Extend Settings class - b75afd9
- [x] 1.4.3 VERIFY: Settings integration test passes - verified (no commit)
- [x] 2.1.1 RED: Test Crawl4AIReaderConfig class - 8f0b93d
- [x] 2.1.2 GREEN: Implement Crawl4AIReaderConfig class - 232abb5
- [x] 2.1.3 REFACTOR: Test configuration validation - a68ff00
- [x] 2.2.1 RED: Tests for health check validation - 96cb51f
- [x] 2.2.2a Implement __init__ with health validation - 5d76dbb
- [x] 2.2.2b Implement _validate_health_sync helper - already implemented in 2.2.2a
- [x] 2.2.2c Verify all health check tests pass - 91ca6ad
- [x] 2.2.3a Add _validate_health async method - e107b45
- [x] 2.2.3b Verify tests still pass after refactor - verified
- [x] V3 Quality checkpoint: initialization complete - verified (no fixes needed)
- [x] 2.3.1a RED: Test deterministic UUID generation - 5cabc13
- [x] 2.3.1b RED: Test different URLs producing different UUIDs - f7a784a
- [x] 2.3.1c RED: Test UUID format validation - f2d3e54
- [x] 2.3.2a GREEN: Implement _generate_document_id method - d5f7b29
- [x] 2.3.2b Verify all document ID tests pass - verified
- [x] 2.3.3a REFACTOR: Enhance docstring with vector_store pattern reference - (pending commit)
- [x] 2.3.3b REFACTOR: Verify tests still pass after docstring changes - (current commit)
- [x] 2.4.1a RED: Write test for complete metadata structure - b0657c8
- [x] 2.4.1b RED: Write tests for missing metadata field defaults - d0fd94d
- [x] 2.4.1c RED: Write test for flat types validation - e0067e4
- [x] 2.4.1d RED: Write test for link counting accuracy - 9efe745
- [x] 2.4.1e RED: Write test for source_url field presence (Issue #17) - c30472e
- [x] 2.4.2a GREEN: Implement _build_metadata method - 74ceafe
- [x] 2.4.3a REFACTOR: Extract link counting into helper function - 7c147fe
- [x] 2.4.3b REFACTOR: Verify tests still pass after refactor - verified
- [x] V4 Quality checkpoint: ID and metadata complete - 18cec91
- [x] 2.5.1a RED: Test successful crawl with fit_markdown - 1a87c15
- [x] 2.5.1b RED: Test markdown fallback to raw_markdown - 5dc1ef2
- [x] 2.5.1c RED: Test missing markdown error - 0510257
- [x] 2.5.1d RED: Test for CrawlResult success=False - 08acb69
- [x] 2.5.1e RED: Test for circuit breaker open state - d6548b9
- [x] 2.5.1f RED: Test for fail_on_error=False returning None - 3d4de3d
- [x] 2.5.2a GREEN: Implement _crawl_single_url core logic - 5911335
- [x] 2.5.3a REFACTOR: Extract internal retry implementation - 256bfc4
- [x] 2.5.3b REFACTOR: Add circuit breaker state logging - (no changes needed, already implemented)
- [x] 2.5.3c REFACTOR: Verify tests still pass after refactor - verified
- [x] 2.6.1a RED/GREEN: Test successful retry after timeout - ab95373
- [x] 2.6.1b GREEN: Test max retries exhausted - ada28ee
- [x] 2.6.1c GREEN: Test for no retry on 4xx errors - (pending commit)
- [x] 2.6.1d GREEN: Test for retry on 5xx errors - fca46d3
- [x] 2.6.1e GREEN: Test for exponential backoff delays verification - 7ca1c8d
- [x] 2.6.2a Add retry loop to _crawl_impl - (acknowledgment commit)
- [x] 2.6.3a Add structured logging for retry attempts - e082592
- [x] 2.6.3b Verify tests still pass after refactor - (current commit)

## Current Task

Awaiting next task

## Learnings

- respx>=0.21.0 already present in dependency-groups.dev for mocking httpx requests in tests (no addition needed)
- BasePydanticReader is preferred over BaseReader for serialization support and IngestionPipeline compatibility
- Crawl4AI provides rich REST API with /crawl, /crawl/stream, /crawl/job endpoints suitable for different use cases
- Response field `markdown.fit_markdown` is optimal for RAG use cases (pre-filtered, high signal-to-noise)
- Project's existing circuit breaker, httpx patterns, and async infrastructure are directly reusable
- SimpleWebPageReader provides excellent reference implementation for HTTP-based readers
- Crawl4AI v0.7.6+ supports webhooks for async jobs, eliminating polling overhead for production workloads
- LlamaIndex Document metadata should include source, title, description, timestamp, and source_type fields
- Async implementation (aload_data) is mandatory for integration with existing async-first pipeline
- Deterministic Document IDs (SHA256 of URL) enable idempotent re-ingestion
- Crawl4AI responses include internal/external link categorization useful for metadata enrichment
- TDD methodology requires explicit RED-GREEN-REFACTOR workflow with 85%+ test coverage target
- User stories must have testable acceptance criteria mappable to unit and integration tests
- Circuit breaker integration is critical NFR to prevent cascading failures during service outages
- Retry logic should distinguish between transient (5xx, timeout) and permanent (4xx) errors
- Metadata must use flat types (str, int, float) only for Qdrant compatibility
- Concurrency control via asyncio.Semaphore prevents resource exhaustion in batch operations
- Integration tests require service health checks with skip-if-unavailable to prevent flaky tests
- Configuration should extend existing Settings class rather than create separate config object
- Missing dependency: httpx must be added explicitly to pyproject.toml (currently implicit via llama-index-core)
- Error handling must support both fail-fast (fail_on_error=True) and continue-on-error (fail_on_error=False) modes
- Structured logging should include operation context (URL, duration, status_code) for observability
- Document ordering preservation in batch operations is important for predictable pipeline behavior
- Circuit breaker wrapping pattern: initialize in __init__, wrap async calls with circuit_breaker.call(lambda: func())
- TEI client pattern provides proven template for retry logic with exponential backoff [1s, 2s, 4s]
- Existing logger.py uses simple human-readable format (not JSON), but includes structured fields via kwargs
- Settings class uses Pydantic BaseSettings with field validators for complex validation (chunk_overlap_percent example)
- Processor.py demonstrates proper metadata building: always convert None to defaults (empty string or 0) for Qdrant
- asyncio.Semaphore pattern: create semaphore, wrap async call in function with "async with semaphore" context manager
- Type hints use modern Python 3.10+ syntax: list[T] not List[T], dict[K,V] not Dict[K,V], X | None not Optional[X]
- Pydantic Field() supports ge/le validation for numeric ranges, description for documentation
- respx library already in dev dependencies for mocking httpx requests in tests (perfect for unit tests)
- BasePydanticReader requires is_remote and class_name properties for LlamaIndex serialization
- Health check validation pattern: synchronous check in __init__ for fail-fast, async check for runtime validation
- Document creation from web content differs from file-based: no file_path metadata, use source URL instead
- PAYLOAD_INDEXES extends cleanly with new fields for different use cases (source_url for web crawl deduplication)
- delete_by_url mirrors delete_by_file pattern exactly: scroll API with filter, collect point IDs, batch delete with retry
- RED phase tests must fail with ImportError or AttributeError to prove test works before implementation
- Settings integration test verifies reader respects CRAWL4AI_BASE_URL configuration field
- Pydantic Field() enables rich field metadata with default values and descriptions for configuration

## Blockers

- None currently

## Next

Task 2.5.1d: RED - Test for CrawlResult success=False

## Task Planning Learnings

- TDD enforcement requires separating every feature into RED-GREEN-REFACTOR sub-tasks (not just test + implementation)
- Each sub-task (RED, GREEN, REFACTOR) must be its own atomic commit for proper TDD verification
- Quality checkpoints should be inserted every 2-3 tasks to catch issues early, not just at phase boundaries
- Test tasks must explicitly verify tests FAIL before implementation (RED phase validation)
- GREEN tasks must implement ONLY minimal code to pass tests, resisting premature optimization
- REFACTOR tasks must verify all tests still pass after code improvements
- Total task count grew from estimated 40 to 89 when properly separating TDD phases
- Phase 2 dominates task count (63 tasks) because every feature requires 3 sub-tasks (RED-GREEN-REFACTOR)
- Integration and E2E tests can be written after implementation (Phase 3), not subject to TDD splitting
- Verification tasks ([VERIFY]) are separate from implementation tasks, used for quality gates
- Order preservation bug found during planning: design.md line 465 filters None values, violating AC-3.4
- Connection pooling should be in initial implementation (not optimization) for proper async client reuse
- Metadata defaults bug: design used isinstance(key, str) instead of checking value type, needs explicit field defaults
- Health check async method was defined but never called, clarified to call in aload_data() for batch validation
- Circuit breaker logging needed explicit reader context, not just assuming internal logging sufficient
- TDD requires 68 total tests (59 unit + 6 integration + 3 e2e) for 85%+ coverage target
- Task descriptions must include exact file paths, not relative or placeholder paths
- Verify commands must be deterministic and runnable (e.g., grep, pytest with specific test name)
- Done when criteria must be explicit and measurable (e.g., "tests pass", not "code works")
- Commit messages follow strict conventional commit format: type(scope): description

## Task Regeneration Learnings (2026-01-15)

- **Issue #15 (UUID Strategy)**: Changed from SHA256 hex string to deterministic UUID (uuid.UUID(bytes=hash[:16])) to match vector_store.py::_generate_point_id() pattern for consistency
- **Issue #16 (Deduplication)**: Reader must delete old documents before re-crawling URLs (matches file watcher's on_modified behavior: delete then process)
- **Issue #17 (source_url Indexing)**: Added source_url field to metadata (same as source) and PAYLOAD_INDEXES for fast deduplication queries
- VectorStoreManager updates moved to Phase 1 setup tasks (not TDD): add source_url index, implement delete_by_url() mirroring delete_by_file pattern
- Deduplication integration in Phase 2 with TDD: enable_deduplication field (default True), vector_store field (optional), _deduplicate_url() method called before crawling
- Total task count increased from 89 to 99 tasks due to VectorStoreManager work and deduplication feature
- Phase 1 grew from 11 to 15 tasks (VectorStoreManager updates + quality checkpoint)
- Phase 2 grew from 63 to 69 tasks (deduplication RED-GREEN-REFACTOR cycle)
- All tasks reference Issues #15, #16, #17 in requirements and design sections for traceability
- delete_by_url() implementation pattern discovered from reading vector_store.py::delete_by_file() (line 713-743): scroll API, batch delete, return count
- source_url must be same value as source field (the URL) to enable deduplication queries
- Deduplication is optional and requires VectorStoreManager instance passed to reader
- Pattern alignment critical: web crawl deduplication mirrors file watcher deduplication for consistency
- Final AC verification (V13) now includes checking Issues #15, #16, #17 are satisfied
- Ruff enforces 88-character line limit; long inline comments need to be shortened for compliance
- Test file structure follows project pattern: comprehensive module docstring, imports (pytest, respx, httpx), fixtures for common config
- TDD methodology requires tests to import from implementation module with explanatory comment about expected RED phase failures
- Test fixtures should cover full spectrum of scenarios: success cases (complete, minimal, fallback), error cases (404, 500, 503), and edge cases (empty content)
- Fixture functions provide convenient access patterns for common test scenarios while maintaining flexibility with fixture constants
- VERIFY tasks confirm test behavior at checkpoints: Settings test fails with ImportError (reader class missing), not AttributeError (Settings field verified)
- Configuration class test structure should verify both field existence (hasattr) and correct types (isinstance) for all required fields
- RED phase tests properly fail with ImportError when target class doesn't exist, confirming test logic before implementation
- Crawl4AIReaderConfig uses separate config class with different field names than reader: base_url (not endpoint_url), timeout (not timeout_seconds), concurrency_limit (not max_concurrent_requests)
- Pydantic ConfigDict with validate_assignment=True enables runtime validation, extra="forbid" prevents unexpected fields
- Configuration defaults: base_url="http://localhost:52004", timeout=30s, max_retries=3, retry_delays=[1.0, 2.0, 4.0], circuit_breaker_threshold=5, circuit_breaker_timeout=60s, concurrency_limit=5

- REFACTOR phase validation tests should PASS immediately when Pydantic Field() constraints already implemented
- ValidationError testing pattern: pytest.raises(ValidationError), assert field name in error message for verification
- Pydantic extra="forbid" validation error messages contain "extra" or "permitted" keywords
- Health check tests use respx.mock decorator for synchronous tests, respx.mock context manager for async tests
- Health check failure should raise ValueError with clear message about service being unreachable
- Circuit breaker and logger initialization tests verify _circuit_breaker and _logger attributes exist after __init__
- RED phase tests properly fail with ImportError when Crawl4AIReader class doesn't exist yet
- Crawl4AIReader class definition requires full field structure even when implementing just __init__ (BasePydanticReader needs all fields declared)
- __init__ implementation naturally includes _validate_health_sync() helper as it's called immediately and they're tightly coupled
- Pydantic private fields (_circuit_breaker, _logger) use | None type hint to allow initialization in __init__ vs class definition
- get_logger() requires module name and log_level parameters for proper structured logging setup
- Task 2.2.2b was already completed as part of 2.2.2a because __init__ and _validate_health_sync are tightly coupled (method called immediately in __init__)
- GREEN phase verification (2.2.2c) confirms all health check tests pass: test_health_check_success, test_health_check_failure, test_circuit_breaker_initialized, test_logger_initialized
- Async _validate_health method mirrors sync version but uses httpx.AsyncClient with async/await for non-blocking runtime validation
- Comprehensive docstrings should include method purpose, return values, and usage examples per Google style
- UUID format validation tests use uuid.UUID() constructor to parse string and verify valid UUID format
- Test pattern for UUID validation: generate ID, parse with UUID(), assert parsed string equals original (confirms format)
- _generate_document_id implementation: SHA256 hash of URL → first 16 bytes → uuid.UUID(bytes=...) → str() - matches vector_store.py pattern exactly
- hashlib and uuid imports required at module level for deterministic ID generation (removed earlier, now re-added)
- GREEN phase verification: All 3 document ID tests pass (test_document_id_deterministic, test_document_id_different_urls, test_document_id_uuid_format)
- REFACTOR phase docstring enhancement: Notes section explicitly references vector_store.py::_generate_point_id() pattern for consistency
- Both ID generators use SHA256 hash → first 16 bytes → UUID pattern, enabling idempotent upsert operations
- Document ID (from URL) + chunk index in downstream processing yields deterministic point IDs matching vector_store behavior
- mock_crawl_result_success fixture provides reusable CrawlResult structure with all fields (url, success, status_code, markdown, metadata, links, crawl_timestamp)
- Metadata test structure: verify all 9 fields present, verify correct values, verify flat types (Qdrant compatible)
- RED phase test properly fails with AttributeError when _build_metadata method doesn't exist yet
- Missing field tests verify graceful defaults: empty string for title/description, 0 for link counts
- Three separate tests for missing fields enable precise failure diagnosis and targeted GREEN phase implementation
- Flat types validation enforces Qdrant payload compatibility: only str, int, float allowed (no None, no nested dicts/lists)
- Test iterates over all metadata values asserting type membership in (str, int, float) with clear error messages per field
- Link counting test uses explicit known quantities (3 internal, 2 external) to verify counting logic accuracy
- CrawlResult.links structure: {"internal": [{"href": "..."}, ...], "external": [{"href": "..."}, ...]}

## Blockers

- None currently

## Next

Task V5: Quality checkpoint - single URL crawling complete

## Additional Learnings (Task 2.4.1e)

- source_url field must exist in metadata and equal source field (both are the URL)
- source_url field enables efficient deduplication queries in Qdrant via indexed payload field
- Test verifies field presence, value equality with source, and string type validation
- RED phase test fails with AttributeError as expected (_build_metadata method doesn't exist yet)

## Additional Learnings (Task 2.4.2a)

- _build_metadata method implementation extracts all 9 metadata fields: source, source_url, title, description, status_code, crawl_timestamp, internal_links_count, external_links_count, source_type
- 'or' operator provides natural default handling: page_metadata.get("title") or "" gives empty string for None/missing values
- Flat types enforcement: str defaults to "", int defaults to 0, no None values allowed
- links.get("internal", []) provides safe default empty list for counting when field missing
- source and source_url both set to url parameter value for deduplication query support
- source_type hardcoded to "web_crawl" as constant discriminator for metadata filtering
- All 7 metadata tests now PASS after implementation: test_metadata_complete, test_metadata_missing_title, test_metadata_missing_description, test_metadata_missing_links, test_metadata_flat_types, test_metadata_links_counting, test_metadata_source_url_present

## Additional Learnings (Task 2.4.3a)

- Extracted link counting logic into _count_links helper function for improved code organization
- Helper function returns tuple[int, int] for (internal_count, external_count)
- REFACTOR verification: All 7 metadata tests still pass after refactoring, confirming behavior preservation
- Code organization best practice: Extract multi-line logic blocks into focused helper functions

## Additional Learnings (Task 2.4.3b)

- Post-refactor verification confirms all 7 metadata tests still pass (100% pass rate)
- REFACTOR phase discipline: always verify tests pass after code restructuring to catch regressions
- Test suite provides confidence for safe refactoring without changing behavior

## Additional Learnings (Task V4)

- Ruff enforces 88-character line limit on all lines including docstring Args sections
- Line continuation in docstrings preserves readability while meeting lint requirements
- Quality checkpoints after major feature implementation catch formatting issues early
- Both ruff and ty checks passed after fixing one line-length violation in _count_links docstring

## Additional Learnings (Task 2.5.1a)

- Single URL crawling test mocks POST /crawl endpoint with respx.post() decorator
- Test structure: mock health check, create reader, mock crawl response, call async method with httpx.AsyncClient
- Async test requires @pytest.mark.asyncio decorator and async with httpx.AsyncClient() context manager
- Mock CrawlResult structure includes all fields: url, success, status_code, markdown (fit_markdown, raw_markdown), metadata, links, crawl_timestamp
- Test verifies Document text equals fit_markdown, metadata includes all 9 fields, and deterministic ID is set
- RED phase test properly fails with AttributeError: 'Crawl4AIReader' object has no attribute '_crawl_single_url'
- Test demonstrates full end-to-end flow: HTTP request → response parsing → Document creation

## Additional Learnings (Task 2.5.1b)

- Fallback test structure: mock response with fit_markdown=None but raw_markdown present
- Test verifies Document.text uses raw_markdown when fit_markdown is None/missing
- Fallback behavior critical for robustness when Crawl4AI doesn't provide cleaned markdown
- Mock response demonstrates edge case handling: partial markdown field structure
- RED phase test fails with AttributeError as expected (method doesn't exist yet)
- Fallback test uses same test structure as success test (respx mock, async client, assertions)

## Additional Learnings (Task 2.5.1c)

- Error handling test verifies ValueError raised when both markdown fields are None
- Edge case: Crawl4AI may return success=True but no extractable content (should be error)
- Test structure: mock response with both fit_markdown and raw_markdown as None
- Expects ValueError with message containing "markdown" or "content"
- RED phase test properly fails with AttributeError (method doesn't exist yet)
- Error path testing crucial for preventing empty documents in pipeline

## Additional Learnings (Task 2.5.1d)

- CrawlResult success=False test verifies RuntimeError raised when crawl operation fails
- Edge case: Crawl4AI returns 200 OK but success=False with error_message (timeout, DNS error, blocked)
- Mock response structure for failure: success=False, status_code=0, error_message string, all other fields None
- Test verifies error message includes both error_message from response and URL for context
- RED phase test properly fails with AttributeError (method doesn't exist yet)
- Failure handling critical for distinguishing network errors from content extraction errors

## Additional Learnings (Task 2.5.1e)

- Circuit breaker open state test manually sets circuit breaker to OPEN to simulate service outage
- CircuitState.OPEN and opened_at=0.0 prevents auto-recovery during test (circuit stays open)
- Test verifies CircuitBreakerError raised when circuit is OPEN (fail-fast pattern)
- Circuit breaker integration prevents cascading failures by rejecting calls immediately
- Test confirms circuit breaker checked BEFORE making HTTP request (no wasted resources)
- RED phase test properly fails with AttributeError: '_crawl_single_url' method doesn't exist yet
- Circuit breaker error message should include "circuit" or "open" keywords for debugging

## Additional Learnings (Task 2.5.1f)

- fail_on_error=False test verifies graceful error handling for batch operations
- Test structure: create reader with fail_on_error=False, mock failed crawl response, assert None returned
- Mock response uses success=False with error_message to simulate real failure scenario
- None return value enables partial success in batch operations (continue processing remaining URLs)
- Critical for bulk crawling where some URLs may be unreachable (DNS failures, timeouts, blocks)
- Test verifies NO exception raised when fail_on_error=False (opposite of success=False test behavior)
- RED phase test properly fails with AttributeError: '_crawl_single_url' method doesn't exist yet
- Graceful failure mode enables continue-on-error pattern for production batch crawling

## Additional Learnings (Task 2.5.2a)

- Implemented _crawl_single_url with nested _crawl_impl() function for retry logic with circuit breaker wrapper
- Circuit breaker wraps entire operation: await self._circuit_breaker.call(_crawl_impl)
- Internal _crawl_impl() function handles retry loop: for attempt in range(self.max_retries + 1)
- POST request payload structure: {"url": url, "crawler_params": {"cache_mode": "BYPASS", "word_count_threshold": 10}}
- response.raise_for_status() checks HTTP status, then check crawl_result.get("success", False) for Crawl4AI operation success
- RuntimeError raised for success=False with error message: f"Crawl failed for {url}: {error_msg}" (includes URL for debugging)
- Markdown extraction with fallback: markdown_data.get("fit_markdown") or markdown_data.get("raw_markdown", "")
- ValueError raised when no markdown content: "No markdown content in response" (fail-fast for empty content)
- Transient errors (TimeoutException, NetworkError, ConnectError) retry with exponential backoff
- 5xx HTTP errors retry with backoff, 4xx errors fail immediately (client errors are permanent)
- fail_on_error flag respected: True raises exception, False logs error and returns None
- All 6 _crawl_single_url tests pass after implementation
- Test fixes required: 3 tests needed fail_on_error=True to properly test exception raising behavior
- Circuit breaker opened_at timestamp fix: time.time() instead of 0.0 to prevent immediate recovery
- asyncio and Document imports added at module level for async sleep and LlamaIndex Document creation

## Additional Learnings (Task 2.5.3a)

- Extracted retry logic into internal _crawl_impl() function within _crawl_single_url() for better code organization
- _crawl_single_url wraps _crawl_impl with circuit breaker: await self._circuit_breaker.call(_crawl_impl)
- Internal implementation handles retry loop, exponential backoff, and transient error handling
- Circuit breaker provides outer fault tolerance layer, retry provides inner transient error handling
- REFACTOR phase verification: All 6 _crawl_single_url tests still pass after extraction (100% pass rate)
- Separation of concerns: retry logic isolated, circuit breaker wrapping explicit

## Additional Learnings (Task 2.5.3b)

- Circuit breaker state logging already implemented in _crawl_single_url (lines 500-518)
- Logs circuit breaker OPEN state after successful call with URL, failure count, and state context
- Logs circuit breaker OPEN state on exception with URL and state context
- Logging follows design.md spec (lines 451-469) for observability of circuit breaker transitions
- No code changes needed, task verification confirmed all 6 tests pass (100% pass rate)

## Additional Learnings (Task 2.5.3c)

- REFACTOR verification checkpoint: All 6 crawl_single_url tests still pass after refactoring (100% pass rate)
- Post-refactor test run confirms extracted retry implementation and circuit breaker logging didn't break existing behavior
- Test categories verified: success, fallback, error handling, circuit breaker integration, fail_on_error modes
- Comprehensive test suite enables confident refactoring without regression risk

## Additional Learnings (Task 2.6.1a)

- Retry logic for timeouts was already implemented in task 2.5.2a (lines 444-466 in crawl4ai_reader.py)
- Test immediately PASSES (GREEN) because retry logic catches TimeoutException and retries with exponential backoff
- respx side_effect requires callback function, not list of responses, when mixing exceptions and responses
- Callback pattern: track call_count, raise exception on first call, return success response on second call
- respx.mock decorator MUST be applied to async tests for mock routes to work properly
- Health check mock must be created BEFORE reader initialization (reader calls health check in __init__)
- Test structure: mock health check → create reader → mock crawl endpoint with side_effect → call method → verify success
- Side effect function signature: def side_effect(request) receives httpx.Request, returns httpx.Response or raises exception
- Test verifies retry behavior: first TimeoutException caught, exponential backoff applied, second request succeeds, Document returned
- Test confirms AC-7.2 (retry on transient errors), FR-10 (exponential backoff), US-7 (crawl robustness)

## Additional Learnings (Task 2.6.1b)

- Max retries exhaustion test verifies exception raised after all retry attempts fail
- Test uses side_effect callback that always raises TimeoutException on every attempt
- Verification includes asserting call_count == 4 (initial + 3 retries as configured with max_retries=3)
- Test immediately PASSES (GREEN) because retry logic already implemented in task 2.5.2a
- Retry exhaustion code path at lines 461-466: logs error after max retries, then raises exception
- Test confirms AC-7.1 (max retries handling), AC-7.7 (exception after exhaustion), FR-10 (exponential backoff)

## Additional Learnings (Task 2.6.1c)

- 4xx no-retry test verifies client errors (permanent errors) do not trigger retry attempts
- Retry logic already implemented in task 2.5.2a (lines 468-491): checks if status_code >= 500 before retrying
- 4xx errors (client errors like 404, 400, 403) fail immediately without retry (only 1 request made)
- 5xx errors (server errors like 500, 503) retry with exponential backoff (transient errors)
- Test immediately PASSES (GREEN) because retry logic correctly distinguishes between 4xx (permanent) and 5xx (transient) errors
- Test verifies HTTPStatusError raised with correct 404 status code and call_count == 1 (no retry)
- Test confirms AC-7.3 (no retry on permanent errors), FR-10 (retry only on transient errors), US-7 (efficient error handling)

## Additional Learnings (Task 2.6.1d)

- 5xx retry test verifies server errors (transient errors) DO trigger retry attempts
- Test uses side_effect callback pattern: 500 on first call, success on second call
- Test immediately PASSES (GREEN) because retry logic already implemented in task 2.5.2a (lines 470-484)
- Lines 470-471 check: if e.response.status_code >= 500 and attempt < self.max_retries (retry 5xx errors)
- Test verifies Document returned after successful retry and call_count == 2 (one retry attempted)
- Server errors (500, 502, 503, 504) are transient and should be retried with exponential backoff
- Test confirms AC-7.3 (retry on transient errors), FR-10 (exponential backoff for server errors), US-7 (error recovery for transient failures)

## Additional Learnings (Task 2.6.1e)

- Exponential backoff test mocks asyncio.sleep to capture delay values passed to sleep() calls
- unittest.mock.patch used to patch asyncio.sleep with custom async function that records delays
- Test verifies delays match configured retry_delays=[1.0, 2.0, 4.0] pattern exactly
- Test immediately PASSES (GREEN) because retry logic already implemented in task 2.5.2a (lines 450-458)
- Side effect callback pattern: timeout on first 3 attempts, success on 4th attempt (initial + 3 retries)
- Mock sleep pattern: async def mock_sleep(delay): sleep_delays.append(delay) captures all delay values
- Patch usage: with patch("asyncio.sleep", side_effect=mock_sleep) replaces asyncio.sleep globally during test
- Test confirms AC-7.2 (exponential backoff retry pattern), NFR-8 (backoff delays), FR-10 (retry delays follow [1.0, 2.0, 4.0])
- Retry logic at lines 450-458 uses self.retry_delays[min(attempt, len(self.retry_delays) - 1)] for delay calculation
- Exponential backoff prevents overwhelming failing services while providing reasonable retry coverage

## Additional Learnings (Task 2.6.2a)

- Task 2.6.2a requested retry loop implementation, but logic was already fully implemented in task 2.5.2a
- Retry implementation at lines 396 (for loop), 444-491 (error handling, backoff, categorization)
- All 4 retry tests pass immediately: timeout retry, max retries, 4xx no-retry, 5xx retry
- TDD RED-GREEN phases sometimes overlap when GREEN implementation includes forward-looking features
- Acknowledgment commit documents that retry logic already complete from earlier task
- Task verification confirmed implementation completeness before marking complete

## Additional Learnings (Task 2.6.3a)

- Enhanced retry logging to include all required context fields: url, attempt, error, delay
- Transient error retry logging (line 454-457): improved message clarity, added delay field to extra dict
- HTTP 5xx error retry logging (line 475-482): improved message clarity, added delay field to extra dict
- Structured logging follows design.md spec (lines 405-410, 427-432) for complete observability
- Logger extra dict enables structured querying: {"url": url, "attempt": attempt, "error": str(e), "delay": delay}
- All 4 retry tests still pass after logging improvements (100% pass rate)
- REFACTOR phase discipline: improve code quality without breaking existing functionality

## Additional Learnings (Task 2.6.3b)

- REFACTOR verification checkpoint: All 4 retry tests still pass after adding structured logging (100% pass rate)
- Post-refactor test run confirms logging enhancements didn't break existing retry behavior
- Test categories verified: timeout retry, max retries exhaustion, 4xx no-retry, 5xx retry
- Comprehensive test suite enables confident code quality improvements without regression risk
