# Implementation Plan: RAG Pipeline

**Branch**: `001-rag-pipeline` | **Date**: 2025-01-11 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/001-rag-pipeline/spec.md`

**Note**: This plan was generated by the `/speckit.plan` command following the workflow defined in `.specify/templates/commands/plan.md`.

## Summary

Build a self-hosted RAG (Retrieval-Augmented Generation) pipeline that provides intelligent web content retrieval through hybrid semantic and keyword search. The system crawls web pages using Crawl4AI, extracts clean markdown content, chunks documents with semantic boundary detection, generates 1024-dimensional embeddings via TEI, and stores documents in PostgreSQL with full-text search while maintaining vectors in Qdrant for similarity matching. Hybrid search combines vector and keyword results using Reciprocal Rank Fusion (RRF) with optional reranking via `gte-reranker-modernbert-base`. The FastAPI-based REST API provides document CRUD, batch operations, search queries, and asynchronous crawl job submission processed by ARQ background workers with domain-level rate limiting and robots.txt compliance.

## Technical Context

**Language/Version**: Python 3.11+
**Primary Dependencies**: FastAPI 0.115+, Pydantic 2.x, SQLAlchemy (async), ARQ 0.26+, httpx 0.28+, Qdrant Client, Redis client
**Storage**: PostgreSQL 15+ (documents + FTS via tsvector), Qdrant 1.9+ (1024-dim vectors, cosine distance), Redis 7+ (cache, queues, rate limiting, circuit breaker state)
**Testing**: pytest, pytest-asyncio, pytest-cov (85%+ coverage required), httpx AsyncClient for API tests, test containers or mocks for external services
**Target Platform**: Linux server (Docker Compose deployment, self-hosted infrastructure)
**Project Type**: Web (Backend API service + background workers)
**Performance Goals**: p95 < 200ms for hybrid search queries, p95 < 100ms for CRUD operations, 10+ pages/second sustained crawl throughput, < 50ms embedding generation per chunk (batched)
**Constraints**: Self-hosted only (no cloud providers), TDD required (tests before code), 85%+ test coverage mandatory, all external calls must use circuit breakers and retry logic, port assignments in 52000+ range
**Scale/Scope**: Knowledge base scale (thousands to hundreds of thousands of documents), not web-scale (billions)

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### Pre-Research Verification

- [x] **Test-Driven Development**: Plan requires tests before implementation (pytest suite, 85%+ coverage)
- [x] **API-First Design**: OpenAPI/Swagger spec required before coding (Phase 1 deliverable)
- [x] **Hybrid Search Excellence**: Vector (Qdrant) + FTS (PostgreSQL) with RRF fusion specified
- [x] **Resilience-First**: Circuit breakers, rate limiting, retry logic, health checks all required
- [x] **Self-Hosted Infrastructure**: PostgreSQL, Redis, Qdrant, TEI, Crawl4AI all self-hosted
- [x] **Security by Default**: URL validation, robots.txt compliance, HMAC webhook signing, rate limiting
- [x] **Observability**: Structured logging with correlation IDs, health endpoints, metrics required
- [x] **Port Assignments**: All services use 52000+ range (Crawl4AI: 52001, Qdrant: 52002, TEI: 52010, Redis: 53379, PostgreSQL: 53432)

### Technology Compliance

**Required Stack Alignment**:
- [x] FastAPI 0.115+ for async REST API
- [x] Pydantic 2.x for schema validation and settings
- [x] PostgreSQL 15+ for document storage and FTS
- [x] Qdrant 1.9+ for vector search (port 52002)
- [x] Redis 7+ for caching, queues, rate limiting
- [x] TEI for embedding generation (port 52010)
- [x] Crawl4AI 0.4+ for web content extraction (port 52001)
- [x] ARQ 0.26+ for background job processing
- [x] httpx 0.28+ for async HTTP requests

**Prohibited Technologies**:
- [x] No cloud-managed databases (using self-hosted PostgreSQL)
- [x] No cloud providers (AWS/GCP/Azure)
- [x] No synchronous HTTP clients (using httpx async)
- [x] No ORM lazy loading (explicit async queries only)
- [x] No global state (dependency injection pattern)

### Quality Gates Commitment

- [x] Unit tests with 85%+ coverage (pytest + pytest-cov)
- [x] Integration tests for external services (mocked or test containers)
- [x] Type checking with mypy --strict
- [x] Linting with ruff check + ruff format
- [x] API contract validation (OpenAPI spec matches implementation)
- [x] Google-style docstrings on all public functions

### Performance Requirements Acknowledgment

- [x] Search latency: p95 < 200ms for hybrid search
- [x] Crawl throughput: 10+ pages/second sustained
- [x] Embedding generation: < 50ms per chunk (batched)
- [x] API response: p95 < 100ms for CRUD operations
- [x] Database queries: All queries indexed, no N+1 patterns

**GATE STATUS**: ✅ PASSED - All constitutional requirements addressed in plan

## Project Structure

### Documentation (this feature)

```text
specs/001-rag-pipeline/
├── plan.md              # This file (/speckit.plan command output)
├── research.md          # Phase 0 output (/speckit.plan command)
├── data-model.md        # Phase 1 output (/speckit.plan command)
├── quickstart.md        # Phase 1 output (/speckit.plan command)
├── contracts/           # Phase 1 output (/speckit.plan command)
│   ├── openapi.yaml     # Complete OpenAPI 3.1 spec
│   ├── crawl.yaml       # Crawl operations contract
│   ├── search.yaml      # Search operations contract
│   ├── documents.yaml   # Document CRUD contract
│   └── admin.yaml       # Admin & monitoring contract
└── tasks.md             # Phase 2 output (/speckit.tasks command - NOT created by /speckit.plan)
```

### Source Code (repository root)

**Structure Decision**: Backend-only web application (API service + background workers). No frontend needed for RAG pipeline MVP.

```text
app/
├── core/
│   ├── abstractions.py       # ABC interfaces (VectorStore, DocumentStore, Embedder, Crawler, Cache)
│   ├── models.py              # Pydantic models (Document, Chunk, CrawlJob, SearchConfig, etc.)
│   ├── config.py              # Settings (Pydantic BaseSettings)
│   └── deps.py                # FastAPI dependency injection
├── api/
│   ├── v1/
│   │   ├── router.py          # Main API router
│   │   ├── crawl.py           # Crawl endpoints
│   │   ├── search.py          # Search endpoints
│   │   ├── documents.py       # Document CRUD
│   │   ├── collections.py     # Collection management
│   │   ├── admin.py           # Admin & monitoring
│   │   └── schemas.py         # Request/response models
│   └── middleware.py          # Auth, logging, CORS
├── services/
│   ├── url_validator.py       # URL validation & normalization (SSRF prevention)
│   ├── crawler.py             # Crawl4AI wrapper (implements Crawler ABC)
│   ├── embedder.py            # TEI wrapper (implements Embedder ABC)
│   ├── search_service.py      # Hybrid search (RRF fusion, reranking)
│   ├── chunker.py             # Semantic boundary detection
│   ├── circuit_breaker.py     # Circuit breaker pattern
│   ├── rate_limiter.py        # Token bucket rate limiting
│   └── webhook_sender.py      # Webhook delivery with retry
├── storage/
│   ├── postgres.py            # PostgreSQL implementation (implements DocumentStore ABC)
│   ├── qdrant.py              # Qdrant implementation (implements VectorStore ABC)
│   └── redis_cache.py         # Redis implementation (implements Cache ABC)
├── workers/
│   ├── crawl_worker.py        # ARQ worker for crawl jobs
│   ├── deep_crawl_worker.py   # Deep crawl orchestration
│   ├── embedding_worker.py    # Background embedding generation
│   └── webhook_worker.py      # Webhook delivery worker
└── main.py                    # FastAPI application entry point

tests/
├── contract/
│   ├── test_openapi_spec.py   # OpenAPI schema validation
│   └── test_api_contracts.py  # Contract testing
├── integration/
│   ├── test_search_flow.py    # End-to-end search scenarios
│   ├── test_crawl_flow.py     # Crawl job lifecycle
│   ├── test_qdrant.py         # Qdrant integration
│   ├── test_postgres.py       # PostgreSQL integration
│   └── test_redis.py          # Redis integration
└── unit/
    ├── test_url_validator.py
    ├── test_chunker.py
    ├── test_circuit_breaker.py
    ├── test_rate_limiter.py
    └── test_rrf_fusion.py

alembic/
├── versions/                  # Database migrations
└── env.py                     # Alembic configuration

docker-compose.yaml            # Service orchestration
pyproject.toml                 # Project dependencies (uv)
.env.example                   # Environment template
README.md                      # Project documentation
```

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

No constitutional violations identified. Plan adheres to all established principles and constraints.

---

## Post-Design Constitution Verification

**Date**: 2025-01-11
**Phase**: After Phase 1 (Design & Contracts)
**Status**: ✅ VERIFIED - All constitutional requirements maintained

### Verification Checklist

- [x] **API-First Design**: OpenAPI 3.1 specification created (`contracts/openapi.yaml`) covering all 30+ endpoints before any implementation code
- [x] **Data Model Complete**: All entities documented in `data-model.md` with Pydantic schemas, validation rules, relationships, and state machines
- [x] **Test Strategy Defined**: TDD workflow documented in `research.md` with RED-GREEN-REFACTOR cycle, 85%+ coverage requirement, test pyramid (70% unit, 25% integration, 5% contract)
- [x] **Hybrid Search Design**: Vector (Qdrant) + FTS (PostgreSQL) with RRF fusion algorithm documented with k=60 default, configurable weights
- [x] **Resilience Patterns**: Circuit breaker, rate limiter, retry logic all designed in `research.md` with Redis-backed state management
- [x] **Self-Hosted Stack**: All services (PostgreSQL, Redis, Qdrant, TEI, Crawl4AI) configured in `docker-compose.yaml` with no cloud dependencies
- [x] **Security by Design**: URL validation (SSRF prevention), robots.txt compliance, HMAC webhook signing all documented in `research.md` and `data-model.md`
- [x] **Observability**: Structured logging (JSON with correlation IDs), health endpoints (`/health`, `/health/ready`), metrics defined in `research.md`
- [x] **Port Assignments**: All services use 52000+ range (Crawl4AI: 52001, Qdrant: 52002, TEI: 52010, API: 52003, Redis: 53379, PostgreSQL: 53432)
- [x] **Technology Compliance**: No prohibited technologies introduced (verified against constitution's prohibited list)

### Design Artifacts Verification

| Artifact | Status | Constitutional Alignment |
|----------|--------|-------------------------|
| `plan.md` | ✅ Complete | Includes Technical Context, Constitution Check, Project Structure |
| `research.md` | ✅ Complete | All technology decisions documented with rationale and alternatives considered |
| `data-model.md` | ✅ Complete | 16 entities with Pydantic schemas, validation rules, relationships, indexes |
| `contracts/openapi.yaml` | ✅ Complete | OpenAPI 3.1 spec for all API endpoints before implementation |
| `quickstart.md` | ✅ Complete | Local development setup with TDD workflow, Docker Compose, testing instructions |
| `CLAUDE.md` | ✅ Updated | Technology stack added to agent context |

### Quality Gates Pre-Commitment

Before implementation begins, the following are committed to:

1. **Unit Tests**: 85%+ coverage using pytest, pytest-asyncio, pytest-cov
2. **Integration Tests**: All external services (Qdrant, Redis, PostgreSQL, TEI, Crawl4AI) with test containers or mocks
3. **Type Checking**: `mypy --strict` with zero errors
4. **Linting**: `ruff check` and `ruff format` with zero issues
5. **API Contract Validation**: Automated tests to ensure OpenAPI spec matches implementation
6. **Google-Style Docstrings**: All public functions, classes, and modules documented

### Performance Commitments

The following performance targets are committed to:

- Search latency: p95 < 200ms for hybrid search
- Crawl throughput: 10+ pages/second sustained
- Embedding generation: < 50ms per chunk (batched)
- API response: p95 < 100ms for CRUD operations
- Database queries: All queries indexed, no N+1 patterns

### Security Commitments

The following security measures are committed to:

- **SSRF Prevention**: URL validation blocks private IP ranges (10.0.0.0/8, 192.168.0.0/16, 172.16.0.0/12, 169.254.0.0/16)
- **Metadata Endpoint Blocking**: 169.254.169.254, metadata.google.internal blocked
- **Robots.txt Compliance**: Automatic checking with 7-day cache
- **HMAC Webhook Signing**: SHA256 signature in `X-Webhook-Signature` header
- **Rate Limiting**: Per-API-key (60 rpm default) and per-domain (1 rps default) limits
- **No Credentials in Code**: All secrets via environment variables (.env)

### Architectural Decisions Log

| Decision | Rationale | Constitutional Principle |
|----------|-----------|-------------------------|
| **RRF Fusion (k=60)** | Robust to score scale differences, no normalization needed | Hybrid Search Excellence |
| **Qdrant INT8 Quantization** | 4x memory reduction, < 5% recall loss | Self-Hosted Infrastructure |
| **PostgreSQL FTS (tsvector)** | Built-in, no Elasticsearch overhead | Self-Hosted Infrastructure |
| **ARQ Job Queue** | Async-native, Redis-backed, simple | Self-Hosted Infrastructure |
| **FastAPI + Pydantic** | Async support, automatic validation, OpenAPI generation | API-First Design |
| **Circuit Breaker (Redis)** | Distributed state, threshold=5, timeout=300s | Resilience-First Architecture |
| **Token Bucket Rate Limiter** | Lua script for atomicity, per-domain and per-key | Resilience-First Architecture |
| **Exponential Backoff with Jitter** | Prevents thundering herd, 1s/2s/4s delays | Resilience-First Architecture |
| **Structured Logging (JSON)** | Correlation IDs, machine-readable, log aggregation-friendly | Observability |

**GATE STATUS**: ✅ PASSED - Design phase complete, all constitutional requirements verified, ready for Phase 2 (Task Generation)

---

## Implementation Readiness

**Status**: ✅ READY FOR IMPLEMENTATION

**Prerequisites Complete**:
- [x] Feature specification defined (`spec.md`)
- [x] Technical research complete (`research.md`)
- [x] Data model designed (`data-model.md`)
- [x] API contracts specified (`contracts/openapi.yaml`)
- [x] Local development guide ready (`quickstart.md`)
- [x] Agent context updated (`CLAUDE.md`)
- [x] Constitutional compliance verified (pre- and post-design)

**Next Command**: `/speckit.tasks` to generate implementation tasks from this plan

**Implementation Branch**: `001-rag-pipeline` (already created)

**Estimated Effort**: Medium-Large (RAG pipeline with 16 entities, 30+ API endpoints, 6 external services)

**Critical Path**:
1. Core models and abstractions (foundation)
2. Storage layer (PostgreSQL, Qdrant, Redis)
3. External service wrappers (Crawl4AI, TEI)
4. Search service (RRF fusion, reranking)
5. API endpoints (FastAPI routes)
6. Background workers (ARQ tasks)
7. Integration tests (end-to-end flows)

**Success Criteria** (from `spec.md`):
- Search results in < 1s for 95% of queries
- 99% uptime for search and document operations
- 90%+ successful crawl rate
- Hybrid search outperforms single-method search in A/B tests
- 95% first-time workflow completion rate
- 10+ concurrent crawl jobs without degradation

---

**Plan Status**: ✅ COMPLETE - Ready for task generation and implementation
